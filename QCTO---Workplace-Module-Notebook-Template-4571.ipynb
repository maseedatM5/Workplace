{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCTO - Workplace Module\n",
    "\n",
    "### Project Title: Avocado Prices and Sales Volume Analysis\n",
    "#### Done By: Muhammad Ahmed Seedat\n",
    "\n",
    "© ExploreAI 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#BC> Background Context</a>\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Data Collection and Description</a>\n",
    "\n",
    "<a href=#three>3. Loading Data </a>\n",
    "\n",
    "<a href=#four>4. Data Cleaning and Filtering</a>\n",
    "\n",
    "<a href=#five>5. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#six>6. Modeling </a>\n",
    "\n",
    "<a href=#seven>7. Evaluation and Validation</a>\n",
    "\n",
    "<a href=#eight>8. Final Model</a>\n",
    "\n",
    "<a href=#nine>9. Conclusion and Future Work</a>\n",
    "\n",
    "<a href=#ten>10. References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " <a id=\"BC\"></a>\n",
    "## **Background Context**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Introduce the project, outline its goals, and explain its significance.\n",
    "* **Details:** Include information about the problem domain, the specific questions or challenges the project aims to address, and any relevant background information that sets the stage for the work.\n",
    "---\n",
    "\n",
    "Agriculture is the backbone of India's economy, contributing significantly to the country's GDP and providing livelihoods for a large portion of the population. Among the various crops cultivated, rice holds a paramount position, being a staple food for millions and a crucial component of India's agricultural exports. This project aims to conduct a comprehensive data analysis of the Indian agricultural sector, with a specific focus on rice production across different states.\n",
    "\n",
    "The primary objective of this project is to analyze the patterns, trends, and factors influencing rice production in India. By leveraging data from the District Level Data (DLD) and Dashboard for Agriculture and Allied-sectors in India, we aim to uncover insights that can help improve productivity, address regional disparities, and inform policy decisions. The analysis will cover key aspects such as yield rates, crop production between 1966 and 2017.\n",
    "\n",
    "For our analysis of rice production across India, we will employ  statistical techniques in section 5 Explority Data Analysis (EDA).\n",
    "\n",
    "The notebook is structured to guide readers through a comprehensive data analysis project. It begins with a Project Overview, which includes an Introduction outlining the context and a Problem Statement to define the issue at hand, followed by the Objectives of the analysis. Next, the Importing Packages section lists the necessary libraries. Loading Data details the process of importing datasets. Data Cleaning addresses how the data is prepared for analysis. The Exploratory Data Analysis (EDA) section provides insights into the data through visualizations and summary statistics. Feature Engineering involves creating new features to improve model performance. The Modeling section describes the algorithms used and their implementation. Model Performance evaluates the effectiveness of the models. The notebook also includes a section on Machine Learning Sprints for further learning, followed by a Conclusion summarizing the findings, and References for sourcing information. \n",
    "\n",
    "Through this project, we hope to provide a detailed understanding of the current state of rice production in India, identify challenges and opportunities, and propose actionable recommendations to enhance the efficiency and sustainability of rice farming. Ultimately, our goal is to support the development of a more resilient and prosperous agricultural sector in India."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#one></a>\n",
    "## **Importing Packages**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Set up the Python environment with necessary libraries and tools.\n",
    "* **Details:** List and import all the Python packages that will be used throughout the project such as Pandas for data manipulation, Matplotlib/Seaborn for visualization, scikit-learn for modeling, etc.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "# Displays output inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#two></a>\n",
    "## **Data Collection and Description**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Describe how the data was collected and provide an overview of its characteristics.\n",
    "* **Details:** Mention sources of the data, the methods used for collection (e.g., APIs, web scraping, datasets from repositories), and a general description of the dataset including size, scope, and types of data available (e.g., numerical, categorical).\n",
    "---\n",
    "\n",
    "The dataset titled \"Avocado Prices and Sales Volume 2015-2023\" was collected from Kaggle, a well-known platform for data science and machine learning datasets¹. The data was gathered using various methods, including web scraping and APIs, to compile comprehensive information on avocado prices and sales volumes across multiple U.S. markets. This dataset spans from 2015 to 2023 and includes both numerical and categorical data. The numerical data covers aspects such as average prices, total volume, and type of avocado (conventional or organic), while the categorical data includes regions and dates. The dataset is extensive, providing a detailed view of market trends over an eight-year period¹.\n",
    "\n",
    "¹: [Kaggle - Avocado Prices and Sales Volume 2015-2023](https://www.kaggle.com/datasets/vakhariapujan/avocado-prices-and-sales-volume-2015-2023)\n",
    "\n",
    "Source: Conversation with Copilot, 2024/09/15\n",
    "(1) Avocado Prices and Sales Volume 2015-2023 - Kaggle. https://www.kaggle.com/datasets/vakhariapujan/avocado-prices-and-sales-volume-2015-2023.\n",
    "(2) Kaggle: Your Home for Data Science. https://www.kaggle.com/datasets/vakhariapujan/avocado-prices-and-sales-volume-2015-2023/download?datasetVersionNumber=3.\n",
    "(3) The Price and Sales of Avocado - Kaggle. https://www.kaggle.com/datasets/alanluo418/avocado-prices-20152019.\n",
    "(4) undefined. https://www.kaggle.com/static/assets/app.js?v=ee89c9be8cfec5b47292:2:2059285.\n",
    "(5) undefined. https://www.kaggle.com/static/assets/app.js?v=ee89c9be8cfec5b47292:2:2056226.\n",
    "(6) undefined. https://www.kaggle.com/static/assets/app.js?v=ee89c9be8cfec5b47292:2:2056331%29.\n",
    "(7) undefined. https://www.kaggle.com/static/assets/app.js?v=ee89c9be8cfec5b47292:2:2054570%29.\n",
    "(8) undefined. https://www.kaggle.com/static/assets/app.js?v=ee89c9be8cfec5b47292:2:2054773%29."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code.\n",
    "\n",
    "df = pd.read_csv(\"Avocado_HassAvocadoBoard_20152023v1.0.1.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#three></a>\n",
    "## **Loading Data**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Load the data into the notebook for manipulation and analysis.\n",
    "* **Details:** Show the code used to load the data and display the first few rows to give a sense of what the raw data looks like.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>AveragePrice</th>\n",
       "      <th>TotalVolume</th>\n",
       "      <th>plu4046</th>\n",
       "      <th>plu4225</th>\n",
       "      <th>plu4770</th>\n",
       "      <th>TotalBags</th>\n",
       "      <th>SmallBags</th>\n",
       "      <th>LargeBags</th>\n",
       "      <th>XLargeBags</th>\n",
       "      <th>type</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.22</td>\n",
       "      <td>40873.28</td>\n",
       "      <td>2819.50</td>\n",
       "      <td>28287.42</td>\n",
       "      <td>49.90</td>\n",
       "      <td>9716.46</td>\n",
       "      <td>9186.93</td>\n",
       "      <td>529.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1373.95</td>\n",
       "      <td>57.42</td>\n",
       "      <td>153.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1162.65</td>\n",
       "      <td>1162.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>Albany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>435021.49</td>\n",
       "      <td>364302.39</td>\n",
       "      <td>23821.16</td>\n",
       "      <td>82.15</td>\n",
       "      <td>46815.79</td>\n",
       "      <td>16707.15</td>\n",
       "      <td>30108.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>Atlanta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.76</td>\n",
       "      <td>3846.69</td>\n",
       "      <td>1500.15</td>\n",
       "      <td>938.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1408.19</td>\n",
       "      <td>1071.35</td>\n",
       "      <td>336.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>organic</td>\n",
       "      <td>Atlanta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>1.08</td>\n",
       "      <td>788025.06</td>\n",
       "      <td>53987.31</td>\n",
       "      <td>552906.04</td>\n",
       "      <td>39995.03</td>\n",
       "      <td>141136.68</td>\n",
       "      <td>137146.07</td>\n",
       "      <td>3990.61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>conventional</td>\n",
       "      <td>BaltimoreWashington</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  AveragePrice  TotalVolume    plu4046    plu4225   plu4770  \\\n",
       "0  2015-01-04          1.22     40873.28    2819.50   28287.42     49.90   \n",
       "1  2015-01-04          1.79      1373.95      57.42     153.88      0.00   \n",
       "2  2015-01-04          1.00    435021.49  364302.39   23821.16     82.15   \n",
       "3  2015-01-04          1.76      3846.69    1500.15     938.35      0.00   \n",
       "4  2015-01-04          1.08    788025.06   53987.31  552906.04  39995.03   \n",
       "\n",
       "   TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\n",
       "0    9716.46    9186.93     529.53         0.0  conventional   \n",
       "1    1162.65    1162.65       0.00         0.0       organic   \n",
       "2   46815.79   16707.15   30108.64         0.0  conventional   \n",
       "3    1408.19    1071.35     336.84         0.0       organic   \n",
       "4  141136.68  137146.07    3990.61         0.0  conventional   \n",
       "\n",
       "                region  \n",
       "0               Albany  \n",
       "1               Albany  \n",
       "2              Atlanta  \n",
       "3              Atlanta  \n",
       "4  BaltimoreWashington  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code.\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#four></a>\n",
    "## **Data Cleaning and Filtering**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Prepare the data for analysis by cleaning and filtering.\n",
    "* **Details:** Include steps for handling missing values, removing outliers, correcting errors, and possibly reducing the data (filtering based on certain criteria or features).\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53415 entries, 0 to 53414\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Date          53415 non-null  object \n",
      " 1   AveragePrice  53415 non-null  float64\n",
      " 2   TotalVolume   53415 non-null  float64\n",
      " 3   plu4046       53415 non-null  float64\n",
      " 4   plu4225       53415 non-null  float64\n",
      " 5   plu4770       53415 non-null  float64\n",
      " 6   TotalBags     53415 non-null  float64\n",
      " 7   SmallBags     41025 non-null  float64\n",
      " 8   LargeBags     41025 non-null  float64\n",
      " 9   XLargeBags    41025 non-null  float64\n",
      " 10  type          53415 non-null  object \n",
      " 11  region        53415 non-null  object \n",
      "dtypes: float64(9), object(3)\n",
      "memory usage: 4.9+ MB\n",
      "['2015-01-04' '2015-01-11' '2015-01-18' '2015-01-25' '2015-02-01'\n",
      " '2015-02-08' '2015-02-15' '2015-02-22' '2015-03-01' '2015-03-08'\n",
      " '2015-03-15' '2015-03-22' '2015-03-29' '2015-04-05' '2015-04-12'\n",
      " '2015-04-19' '2015-04-26' '2015-05-03' '2015-05-10' '2015-05-17'\n",
      " '2015-05-24' '2015-05-31' '2015-06-07' '2015-06-14' '2015-06-21'\n",
      " '2015-06-28' '2015-07-05' '2015-07-12' '2015-07-19' '2015-07-26'\n",
      " '2015-08-02' '2015-08-09' '2015-08-16' '2015-08-23' '2015-08-30'\n",
      " '2015-09-06' '2015-09-13' '2015-09-20' '2015-09-27' '2015-10-04'\n",
      " '2015-10-11' '2015-10-18' '2015-10-25' '2015-11-01' '2015-11-08'\n",
      " '2015-11-15' '2015-11-22' '2015-11-29' '2015-12-06' '2015-12-13'\n",
      " '2015-12-20' '2015-12-27' '2016-01-03' '2016-01-10' '2016-01-17'\n",
      " '2016-01-24' '2016-01-31' '2016-02-07' '2016-02-14' '2016-02-21'\n",
      " '2016-02-28' '2016-03-06' '2016-03-13' '2016-03-20' '2016-03-27'\n",
      " '2016-04-03' '2016-04-10' '2016-04-17' '2016-04-24' '2016-05-01'\n",
      " '2016-05-08' '2016-05-15' '2016-05-22' '2016-05-29' '2016-06-05'\n",
      " '2016-06-12' '2016-06-19' '2016-06-26' '2016-07-03' '2016-07-10'\n",
      " '2016-07-17' '2016-07-24' '2016-07-31' '2016-08-07' '2016-08-14'\n",
      " '2016-08-21' '2016-08-28' '2016-09-04' '2016-09-11' '2016-09-18'\n",
      " '2016-09-25' '2016-10-02' '2016-10-09' '2016-10-16' '2016-10-23'\n",
      " '2016-10-30' '2016-11-06' '2016-11-13' '2016-11-20' '2016-11-27'\n",
      " '2016-12-04' '2016-12-11' '2016-12-18' '2016-12-25' '2017-01-01'\n",
      " '2017-01-08' '2017-01-15' '2017-01-22' '2017-01-29' '2017-02-05'\n",
      " '2017-02-12' '2017-02-19' '2017-02-26' '2017-03-05' '2017-03-12'\n",
      " '2017-03-19' '2017-03-26' '2017-04-02' '2017-04-09' '2017-04-16'\n",
      " '2017-04-23' '2017-04-30' '2017-05-07' '2017-05-14' '2017-05-21'\n",
      " '2017-05-28' '2017-06-04' '2017-06-11' '2017-06-18' '2017-06-25'\n",
      " '2017-07-02' '2017-07-09' '2017-07-16' '2017-07-23' '2017-07-30'\n",
      " '2017-08-06' '2017-08-13' '2017-08-20' '2017-08-27' '2017-09-03'\n",
      " '2017-09-10' '2017-09-17' '2017-09-24' '2017-10-01' '2017-10-08'\n",
      " '2017-10-15' '2017-10-22' '2017-10-29' '2017-11-05' '2017-11-12'\n",
      " '2017-11-19' '2017-11-26' '2017-12-03' '2017-12-10' '2017-12-17'\n",
      " '2017-12-24' '2017-12-31' '2018-01-08' '2018-01-14' '2018-01-21'\n",
      " '2018-01-28' '2018-02-04' '2018-02-11' '2018-02-18' '2018-02-25'\n",
      " '2018-03-04' '2018-03-11' '2018-03-18' '2018-03-25' '2018-04-01'\n",
      " '2018-04-08' '2018-04-15' '2018-04-22' '2018-04-29' '2018-05-06'\n",
      " '2018-05-13' '2018-05-20' '2018-05-27' '2018-06-03' '2018-06-10'\n",
      " '2018-06-17' '2018-06-24' '2018-07-01' '2018-07-08' '2018-07-15'\n",
      " '2018-07-22' '2018-07-29' '2018-08-05' '2018-08-12' '2018-08-19'\n",
      " '2018-08-26' '2018-09-02' '2018-09-09' '2018-09-16' '2018-09-23'\n",
      " '2018-09-30' '2018-10-07' '2018-10-14' '2018-10-21' '2018-10-28'\n",
      " '2018-11-04' '2018-11-11' '2018-11-18' '2018-11-25' '2018-12-02'\n",
      " '2018-12-09' '2018-12-16' '2018-12-23' '2018-12-30' '2019-01-07'\n",
      " '2019-01-13' '2019-01-20' '2019-01-27' '2019-02-03' '2019-02-10'\n",
      " '2019-02-17' '2019-02-24' '2019-03-03' '2019-03-10' '2019-03-17'\n",
      " '2019-03-24' '2019-03-31' '2019-04-07' '2019-04-14' '2019-04-21'\n",
      " '2019-04-28' '2019-05-05' '2019-05-12' '2019-05-19' '2019-05-26'\n",
      " '2019-06-02' '2019-06-09' '2019-06-16' '2019-06-23' '2019-06-30'\n",
      " '2019-07-07' '2019-07-14' '2019-07-21' '2019-07-28' '2019-08-04'\n",
      " '2019-08-11' '2019-08-18' '2019-08-25' '2019-09-01' '2019-09-08'\n",
      " '2019-09-15' '2019-09-22' '2019-09-29' '2019-10-06' '2019-10-13'\n",
      " '2019-10-20' '2019-10-27' '2019-11-03' '2019-11-10' '2019-11-17'\n",
      " '2019-11-24' '2019-12-01' '2019-12-08' '2019-12-15' '2019-12-22'\n",
      " '2019-12-29' '2020-01-06' '2020-01-12' '2020-01-19' '2020-01-26'\n",
      " '2020-02-02' '2020-02-09' '2020-02-16' '2020-02-23' '2020-03-01'\n",
      " '2020-03-08' '2020-03-15' '2020-03-22' '2020-03-29' '2020-04-05'\n",
      " '2020-04-12' '2020-04-19' '2020-04-26' '2020-05-03' '2020-05-10'\n",
      " '2020-05-17' '2020-05-24' '2020-05-31' '2020-06-07' '2020-06-14'\n",
      " '2020-06-21' '2020-06-28' '2020-07-05' '2020-07-12' '2020-07-19'\n",
      " '2020-07-26' '2020-08-02' '2020-08-09' '2020-08-16' '2020-08-23'\n",
      " '2020-08-30' '2020-09-06' '2020-09-13' '2020-09-20' '2020-09-27'\n",
      " '2020-10-04' '2020-10-11' '2020-10-18' '2020-10-25' '2020-11-01'\n",
      " '2020-11-08' '2020-11-15' '2020-11-22' '2020-11-29' '2020-12-06'\n",
      " '2020-12-13' '2020-12-20' '2020-12-27' '2021-01-04' '2021-01-10'\n",
      " '2021-01-17' '2021-01-24' '2021-01-31' '2021-02-07' '2021-02-14'\n",
      " '2021-02-21' '2021-02-28' '2021-03-07' '2021-03-14' '2021-03-21'\n",
      " '2021-03-28' '2021-04-04' '2021-04-11' '2021-04-18' '2021-04-25'\n",
      " '2021-05-02' '2021-05-09' '2021-05-16' '2021-05-23' '2021-05-30'\n",
      " '2021-06-06' '2021-06-13' '2021-06-20' '2021-06-27' '2021-07-04'\n",
      " '2021-07-11' '2021-07-18' '2021-07-25' '2021-08-01' '2021-08-08'\n",
      " '2021-08-15' '2021-08-22' '2021-08-29' '2021-09-05' '2021-09-12'\n",
      " '2021-09-19' '2021-09-26' '2021-10-03' '2021-10-10' '2021-10-17'\n",
      " '2021-10-24' '2021-10-31' '2021-11-07' '2021-11-14' '2021-11-21'\n",
      " '2021-11-28' '2021-12-05' '2021-12-12' '2021-12-19' '2021-12-26'\n",
      " '2022-01-02' '2022-01-10' '2022-01-16' '2022-01-23' '2022-01-30'\n",
      " '2022-02-06' '2022-02-13' '2022-02-20' '2022-02-27' '2022-03-06'\n",
      " '2022-03-13' '2022-03-20' '2022-03-27' '2022-04-03' '2022-04-10'\n",
      " '2022-04-17' '2022-04-24' '2022-05-01' '2022-05-08' '2022-05-15'\n",
      " '2022-05-22' '2022-05-29' '2022-06-05' '2022-06-12' '2022-06-19'\n",
      " '2022-06-26' '2022-07-03' '2022-07-10' '2022-07-17' '2022-07-24'\n",
      " '2022-07-31' '2022-08-07' '2022-08-14' '2022-08-21' '2022-08-28'\n",
      " '2022-09-04' '2022-09-11' '2022-09-18' '2022-09-25' '2022-10-02'\n",
      " '2022-10-09' '2022-10-16' '2022-10-23' '2022-10-30' '2022-11-06'\n",
      " '2022-11-13' '2022-11-20' '2022-11-27' '2022-12-04' '2022-12-11'\n",
      " '2022-12-18' '2022-12-25' '2023-01-01' '2023-01-09' '2023-01-15'\n",
      " '2023-01-22' '2023-01-29' '2023-02-05' '2023-02-12' '2023-02-19'\n",
      " '2023-02-26' '2023-03-05' '2023-03-12' '2023-03-19' '2023-03-26'\n",
      " '2023-04-02' '2023-04-09' '2023-04-16' '2023-04-23' '2023-04-30'\n",
      " '2023-05-07' '2023-05-14' '2023-05-21' '2023-05-28' '2023-06-04'\n",
      " '2023-06-11' '2023-06-18' '2023-06-25' '2023-07-02' '2023-07-09'\n",
      " '2023-07-16' '2023-07-23' '2023-07-30' '2023-08-06' '2023-08-13'\n",
      " '2023-08-20' '2023-08-27' '2023-09-03' '2023-09-10' '2023-09-17'\n",
      " '2023-09-24' '2023-10-01' '2023-10-08' '2023-10-15' '2023-10-22'\n",
      " '2023-10-29' '2023-11-05' '2023-11-12' '2023-11-19' '2023-11-26'\n",
      " '2023-12-03']\n",
      "Dist Code unique counts 2\n",
      "Dist Name unique counts 60\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'unique' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m var \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDist Name unique counts \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m unique, nunique, value_counts()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# look for columns that are numerical\u001b[39;00m\n\u001b[1;32m     20\u001b[0m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'unique' is not defined"
     ]
    }
   ],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code.\n",
    "df_copy = df.copy()\n",
    "df_copy.shape\n",
    "df_copy.info()\n",
    "# Data Cleaning\n",
    "def check_null_values(df_copy):\n",
    "    \"\"\"\n",
    "    Print the count of null values for each column in a DataFrame.\n",
    "\n",
    "    This function iterates through each column in the DataFrame to check for the presence of null values.\n",
    "    If a column contains null values, it prints the column name along with the number of null values.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame to check for null values.\n",
    "\n",
    "    Returns:\n",
    "    None: This function does not return a value; it only prints information.\n",
    "    \"\"\"\n",
    "    for column in df_copy:\n",
    "        if df_copy[column].isnull().any():\n",
    "            print('{0} has {1} null values'.format(column, df_copy[column].isnull().sum()))\n",
    "\n",
    "# dup null\n",
    "check_null_values(df_copy)\n",
    "#check dup\n",
    "def count_duplicate_rows(df_copy):\n",
    "    \"\"\"\n",
    "    Count the number of duplicate rows in a DataFrame.\n",
    "\n",
    "    This function calculates the total number of duplicate rows in the DataFrame by calling the `duplicated` method,\n",
    "    which marks duplicates as `True`, and then sums these cases.\n",
    "\n",
    "    Parameters:\n",
    "    df_copy (pandas.DataFrame): The DataFrame to check for duplicates.\n",
    "\n",
    "    Returns:\n",
    "    int: The count of duplicate rows.\n",
    "    \"\"\"\n",
    "    duplicate_count = df_copy.duplicated().sum()\n",
    "    return duplicate_count\n",
    "# run dup check func\n",
    "count_duplicate_rows(df_copy)\n",
    "\n",
    "## preprocess function\n",
    "def preprocess_data(df_copy):\n",
    "    \"\"\"\n",
    "    Perform basic data cleaning on the DataFrame.\n",
    "\n",
    "    This includes:\n",
    "    - Setting missing 'seasons' to 0.\n",
    "    - Filling missing IMDb scores and votes with the mean and median, respectively.\n",
    "    - Filling missing TMDB popularity and scores with the median and mean, respectively.\n",
    "    - Dropping columns with many null values or that are not required.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assign zero seasons to movies where the number of seasons is missing\n",
    "    df_copy['seasons'] = df_copy['seasons'].fillna(0)\n",
    "    \n",
    "    # Fill missing IMDb scores with the mean score\n",
    "    df_copy['imdb_score'] = df_copy['imdb_score'].fillna(df_copy['imdb_score'].mean())\n",
    "    \n",
    "    # Fill missing IMDb votes with the median number of votes\n",
    "    df_copy['imdb_votes'] = df_copy['imdb_votes'].fillna(df_copy['imdb_votes'].median())\n",
    "    \n",
    "    # Fill missing TMDB popularity scores with the median popularity\n",
    "    df_copy['tmdb_popularity'] = df_copy['tmdb_popularity'].fillna(df_copy['tmdb_popularity'].median())\n",
    "    \n",
    "    # Fill missing TMDB scores with the mean score\n",
    "    df_copy['tmdb_score'] = df_copy['tmdb_score'].fillna(df_copy['tmdb_score'].mean())\n",
    "    \n",
    "    # Drop columns that are not required or have many null values\n",
    "    df_copy = df_copy.drop(['imdb_id', 'description', 'age_certification'], axis=1)\n",
    "    \n",
    "    return df_copy\n",
    "###\n",
    "preprocess_data(df_copy.head(2))\n",
    "\n",
    "# examine the datatypes of the columns\n",
    "df.info()\n",
    "\n",
    "# look for columns that are categorical\n",
    "df.select_dtypes(include=['object', 'category'])\n",
    "print(df[\"Date\"].unique())\n",
    "df[\"Date\"].nunique()\n",
    "df[\"Date\"].value_counts()\n",
    "\n",
    "var = df[\"type\"].nunique()\n",
    "print(f\"Dist Code unique counts {var}\")\n",
    "var = df[\"region\"].nunique()\n",
    "print(f\"Dist Name unique counts {var}\")\n",
    "\n",
    "unique, nunique, value_counts()\n",
    "\n",
    "# look for columns that are numerical\n",
    "df.select_dtypes(include=['int64'])\n",
    "df.select_dtypes(include=['float64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#five></a>\n",
    "## **Exploratory Data Analysis (EDA)**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Explore and visualize the data to uncover patterns, trends, and relationships.\n",
    "* **Details:** Use statistics and visualizations to explore the data. This may include histograms, box plots, scatter plots, and correlation matrices. Discuss any significant findings.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code.\n",
    "\n",
    "\n",
    "def filter_by_runtime(df_copy, min_runtime, max_runtime):\n",
    "    \"\"\"\n",
    "    Filter a DataFrame based on a specified runtime range and return the first two entries.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pandas.DataFrame): The DataFrame to sort and filter.\n",
    "    min_runtime (int): The minimum runtime threshold.\n",
    "    max_runtime (int): The maximum runtime threshold.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The first five entries of the filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Sort the DataFrame by 'runtime'\n",
    "    sorted_df = df_copy.sort_values('runtime')\n",
    "\n",
    "    # Filter the DataFrame for entries with a runtime within the specified range\n",
    "    filtered_selection = sorted_df[\n",
    "        (sorted_df['runtime'] > min_runtime) & (sorted_df['runtime'] <= max_runtime)\n",
    "    ]\n",
    "\n",
    "    # Return the first five entries of the filtered selection\n",
    "    return filtered_selection.head(2)\n",
    "\n",
    "filter_by_runtime(df_copy, 36, 116)\n",
    "\n",
    "\n",
    "# Get the count of each type in the dataset\n",
    "type_count = df_copy['type'].value_counts()\n",
    "\n",
    "# Display the top 5 most common types\n",
    "type_count.head()\n",
    "\n",
    "\n",
    "# Setup countplot\n",
    "sns.countplot(\n",
    "    data=df_copy,\n",
    "    x='type',\n",
    "    order=df_copy['type'].value_counts().index\n",
    ")\n",
    "\n",
    "# Set the title of the plot with an adjusted position for better readability\n",
    "plt.title('Comparing Netflix Media Types', y=1.1)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def plot_release_by_year(df_copy):\n",
    "    \"\"\"\n",
    "    Generate a countplot displaying the number of movies and shows released each year.\n",
    "\n",
    "    Parameters:\n",
    "    df_copy (pandas.DataFrame): DataFrame containing the data with 'release_year' and 'type' columns.\n",
    "\n",
    "    The function creates a figure, sets up a countplot with 'release_year' on the y-axis and colors by 'type',\n",
    "    adjusts the y-axis limits, sets a title, and displays the plot.\n",
    "    \"\"\"\n",
    "    # Create a figure with specified size\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # Create a countplot for releases per year differentiated by type\n",
    "    sns.countplot(data=df_copy, y='release_year', hue='type')\n",
    "\n",
    "    # Set the limits for the y-axis\n",
    "    plt.ylim(20, 65)\n",
    "\n",
    "    # Set the title of the plot\n",
    "    plt.title('How many movies/shows are released each year?')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "plot_release_by_year(df_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#six></a>\n",
    "## **Modeling**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Develop and train predictive or statistical models.\n",
    "* **Details:** Describe the choice of models, feature selection and engineering processes, and show how the models are trained. Include code for setting up the models and explanations of the model parameters.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#seven></a>\n",
    "## **Evaluation and Validation**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Evaluate and validate the effectiveness and accuracy of the models.\n",
    "* **Details:** Present metrics used to evaluate the models, such as accuracy, precision, recall, F1-score, etc. Discuss validation techniques employed, such as cross-validation or train/test split.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#eight></a>\n",
    "## **Final Model**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Present the final model and its performance.\n",
    "* **Details:** Highlight the best-performing model and discuss its configuration, performance, and why it was chosen over others.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#nine></a>\n",
    "## **Conclusion and Future Work**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Summarize the findings and discuss future directions.\n",
    "* **Details:** Conclude with a summary of the results, insights gained, limitations of the study, and suggestions for future projects or improvements in methodology or data collection.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#ten></a>\n",
    "## **References**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Provide citations and sources of external content.\n",
    "* **Details:** List all the references and sources consulted during the project, including data sources, research papers, and documentation for tools and libraries used.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Sections to Consider\n",
    "\n",
    "* ### Appendix: \n",
    "For any additional code, detailed tables, or extended data visualizations that are supplementary to the main content.\n",
    "\n",
    "* ### Contributors: \n",
    "If this is a group project, list the contributors and their roles or contributions to the project.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
